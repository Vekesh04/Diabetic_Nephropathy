import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random
import warnings 
warnings.filterwarnings('ignore')
data = pd.read_csv('/Users/vekesh/Downloads/Summer Interenship /Diabetic nephropathy.csv')
data.head()
data.info()
data.describe()
data.isnull().sum()
data.duplicated().sum()
numeric_data = data.select_dtypes(include=np.number)
corr_matrix = numeric_data.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Features')
plt.show()
for col in data.select_dtypes(include=np.number):
    data[col] = data[col].fillna(data[col].mean())
for col in data.select_dtypes(exclude=np.number):
    data[col] = data[col].fillna(data[col].mode()[0])
from sklearn.preprocessing import StandardScaler, LabelEncoder
X = data.drop('Stages', axis=1)  
y = data['Stages']
numerical_cols = X.select_dtypes(include=np.number).columns
categorical_cols = X.select_dtypes(exclude=np.number).columns
scaler = StandardScaler()
X[numerical_cols] = scaler.fit_transform(X[numerical_cols])
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le
key_features = ['Serum creatinine', 'Albumin']
X_selected = X[key_features]
X = data.drop('Stages', axis=1)
y = data['Stages']
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 80% train, 20% test
categorical_features = X_train.select_dtypes(include=['object']).columns
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') 
encoder.fit(X_train[categorical_features])
X_train_encoded = pd.DataFrame(encoder.transform(X_train[categorical_features]),
                                 columns=encoder.get_feature_names_out(categorical_features),
                                 index=X_train.index)
X_test_encoded = pd.DataFrame(encoder.transform(X_test[categorical_features]),
                                columns=encoder.get_feature_names_out(categorical_features),
                                index=X_test.index)
X_train = pd.concat([X_train.drop(categorical_features, axis=1), X_train_encoded], axis=1)
X_test = pd.concat([X_test.drop(categorical_features, axis=1), X_test_encoded], axis=1)
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train, y_train)
rf_predictions = rf_classifier.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_predictions)
print(f"Random Forest Accuracy: {rf_accuracy}")
print(classification_report(y_test, rf_predictions))

svm_classifier = SVC(random_state=42)
svm_classifier.fit(X_train, y_train)
svm_predictions = svm_classifier.predict(X_test)
svm_accuracy = accuracy_score(y_test, svm_predictions)
print(f"SVM Accuracy: {svm_accuracy}")
print(classification_report(y_test, svm_predictions))

lr_classifier = LogisticRegression(random_state=42, max_iter=1000)
lr_classifier.fit(X_train, y_train)
lr_predictions = lr_classifier.predict(X_test)
lr_accuracy = accuracy_score(y_test, lr_predictions)
print(f"Logistic Regression Accuracy: {lr_accuracy}")
print(classification_report(y_test, lr_predictions))
from sklearn.model_selection import GridSearchCV
rf_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}
rf_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5)
rf_grid_search.fit(X_train, y_train)
print("Best parameters for Random Forest:", rf_grid_search.best_params_)
best_rf_classifier = rf_grid_search.best_estimator_
rf_predictions = best_rf_classifier.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_predictions)
print(f"Tuned Random Forest Accuracy: {rf_accuracy}")
print(classification_report(y_test, rf_predictions))
svm_param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}
svm_grid_search = GridSearchCV(SVC(random_state=42), svm_param_grid, cv=5)
svm_grid_search.fit(X_train, y_train)
print("Best parameters for SVM:", svm_grid_search.best_params_)
best_svm_classifier = svm_grid_search.best_estimator_
svm_predictions = best_svm_classifier.predict(X_test)
svm_accuracy = accuracy_score(y_test, svm_predictions)
print(f"Tuned SVM Accuracy: {svm_accuracy}")
print(classification_report(y_test, svm_predictions))
lr_param_grid = {
    'C': [0.1, 1, 10],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga']
}
lr_grid_search = GridSearchCV(LogisticRegression(random_state=42, max_iter=1000), lr_param_grid, cv=5)
r_grid_search.fit(X_train, y_train)
print("Best parameters for Logistic Regression:", lr_grid_search.best_params_)
best_lr_classifier = lr_grid_search.best_estimator_
lr_predictions = best_lr_classifier.predict(X_test)
lr_accuracy = accuracy_score(y_test, lr_predictions)
print(f"Tuned Logistic Regression Accuracy: {lr_accuracy}")
print(classification_report(y_test, lr_predictions))
rf_predictions = best_rf_classifier.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_predictions)
print(f"Random Forest Test Accuracy: {rf_accuracy}")
print(classification_report(y_test, rf_predictions))

svm_predictions = best_svm_classifier.predict(X_test)
svm_accuracy = accuracy_score(y_test, svm_predictions)
print(f"SVM Test Accuracy: {svm_accuracy}")
print(classification_report(y_test, svm_predictions))

lr_predictions = best_lr_classifier.predict(X_test)
lr_accuracy = accuracy_score(y_test, lr_predictions)
print(f"Logistic Regression Test Accuracy: {lr_accuracy}")
print(classification_report(y_test, lr_predictions))
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def evaluate_model(model, X_test, y_test):
    """Evaluates a given model using various metrics."""
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    return accuracy, precision, recall, f1

rf_accuracy, rf_precision, rf_recall, rf_f1 = evaluate_model(best_rf_classifier, X_test, y_test)
svm_accuracy, svm_precision, svm_recall, svm_f1 = evaluate_model(best_svm_classifier, X_test, y_test)
lr_accuracy, lr_precision, lr_recall, lr_f1 = evaluate_model(best_lr_classifier, X_test, y_test)


print("Random Forest:")
print(f"Accuracy: {rf_accuracy:.4f}")
print(f"Precision: {rf_precision:.4f}")
print(f"Recall: {rf_recall:.4f}")
print(f"F1-score: {rf_f1:.4f}")
print("\nSVM:")
print(f"Accuracy: {svm_accuracy:.4f}")
print(f"Precision: {svm_precision:.4f}")
print(f"Recall: {svm_recall:.4f}")
print(f"F1-score: {svm_f1:.4f}")


print("\nLogistic Regression:")
print(f"Accuracy: {lr_accuracy:.4f}")
print(f"Precision: {lr_precision:.4f}")
print(f"Recall: {lr_recall:.4f}")
print(f"F1-score: {lr_f1:.4f}")
rf_predictions = best_rf_classifier.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_predictions)
print(f"Random Forest Test Accuracy: {rf_accuracy}")

!pip install shap

import shap
explainer = shap.TreeExplainer(best_rf_classifier)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, plot_type="bar")

import numpy as np
shap_importances = np.abs(shap_values).mean(0)
importance_df = pd.DataFrame([X_test.columns.tolist(), shap_importances.tolist()]).T
importance_df.columns = ['column_name', 'shap_importance']
importance_df = importance_df.sort_values('shap_importance', ascending=False)
importance_df
explainer = shap.TreeExplainer(best_rf_classifier)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, plot_type="dot")
